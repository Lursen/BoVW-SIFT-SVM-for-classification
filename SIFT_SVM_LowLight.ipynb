{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIFT-SVM_LowLight.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNOitM56N33u3uwbyeMH+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lursen/BoVW-SIFT-SVM-for-classification/blob/main/SIFT_SVM_LowLight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-oI14I3drO2"
      },
      "source": [
        "!pip uninstall opencv-contrib-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v31Olj62cmLo"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PwgZhhQTPMQ"
      },
      "source": [
        "import glob, os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DbXJpPWN_Tr"
      },
      "source": [
        "# Loading data\n",
        "def readImages(dir, categories):\n",
        "  data = {}\n",
        "  for category in categories:\n",
        "      cur_dir = rootdir+category\n",
        "      imgs = []\n",
        "      for img in os.listdir(cur_dir):\n",
        "          if img is not None:\n",
        "            #image = Image.open(cur_dir+'/'+img)\n",
        "            image = cv2.imread(cur_dir + \"/\" + img,0)\n",
        "            imgs.append(image)\n",
        "            print(img)\n",
        "      data[category] = imgs\n",
        "  return data\n",
        "\n",
        "# Creating training/test sets\n",
        "def splitImages(data, train_size):\n",
        "  # train/test split\n",
        "  train = {}\n",
        "  test = {}\n",
        "  for key in data:\n",
        "    cat_train = []\n",
        "    cat_test = []\n",
        "    random.shuffle(data[key])\n",
        "    i = 0\n",
        "    for value in data[key]:\n",
        "      if (i < int(len(data[key])*0.8)):\n",
        "        cat_train.append(value)\n",
        "      else:\n",
        "        cat_test.append(value)\n",
        "      i = i+1\n",
        "    train[key] = cat_train\n",
        "    test[key] = cat_test\n",
        "  return train, test\n",
        "  \n",
        "# SIFT features\n",
        "def getSiftFeatures(data):\n",
        "  sift_vectors = {}\n",
        "  descriptor_list = []\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  for key,value in data.items():\n",
        "    descriptors = []\n",
        "    print(key)\n",
        "    for img in value:\n",
        "        keys, descs = sift.detectAndCompute(img, None)\n",
        "        if(descs is not None):\n",
        "          descriptors.append(descs)\n",
        "          descriptor_list.extend(descs)\n",
        "    sift_vectors[key] = descriptors\n",
        "  return descriptor_list, sift_vectors\n",
        "\n",
        "# K-means clusterization\n",
        "def Kmeans(k, descriptor_list):\n",
        "  kmeans = MiniBatchKMeans(n_clusters = k, init='k-means++', batch_size=1000, \n",
        "                           n_init=10, max_no_improvement=10, verbose=True).fit(descriptor_list)\n",
        "  return kmeans\n",
        "\n",
        "# Vector quantization\n",
        "def getHistograms(descriptors, kmeans):\n",
        "    dict_feature = {}\n",
        "    for key,value in descriptors.items():\n",
        "        category = []\n",
        "        for img in value:\n",
        "            histogram = np.zeros(len(kmeans.cluster_centers_))\n",
        "            for each_feature in img:\n",
        "              feature = (each_feature.reshape(1, 128))[0].astype('float')\n",
        "              feature = feature.reshape(1, 128)\n",
        "              idx = kmeans.predict(feature)\n",
        "              histogram[idx] += 1\n",
        "            category.append(histogram)\n",
        "        dict_feature[key] = category\n",
        "    return dict_feature\n",
        "\n",
        "# Feature normalization\n",
        "def featureNormalization(scale, im_features):\n",
        "  scale = StandardScaler().fit(im_features)        \n",
        "  im_features = scale.transform(im_features)\n",
        "  return im_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "\n",
        "os.makedirs(\"dataset\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1GZqHFzTLDI-1rcOctHdf-c16VgagWocd\"\n",
        "output = \"dataset/ExDark.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"dataset/ExDark.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"dataset\")"
      ],
      "metadata": {
        "id": "N2TQ9H4ASBLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['Bicycle','Boat','Bottle','Bus','Car','Cat','Chair','Cup','Dog','Motorbike','People','Table']\n",
        "rootdir = \"/content/dataset/\"\n",
        "\n",
        "# Load data\n",
        "data = readImages(rootdir, categories)\n",
        "train, test = splitImages(data, train_size=0.7)"
      ],
      "metadata": {
        "id": "TzXkigscQKXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmOATg_1XtUG"
      },
      "source": [
        "# Get SIFT features\n",
        "descriptor_list, train_features  = getSiftFeatures(train) \n",
        "test_features = getSiftFeatures(test)[1] \n",
        "\n",
        "# Perform K-means clusterization\n",
        "k = 100 \n",
        "kmeans = Kmeans(k, descriptor_list) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Vq3DVlN_QR"
      },
      "source": [
        "# Creates histograms for train/test data    \n",
        "hist_train = getHistograms((train_features), kmeans) \n",
        "hist_test = getHistograms((test_features), kmeans) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test  = []\n",
        "y_test  = []\n",
        "\n",
        "dict_cat = {'Bicycle':0,'Boat':1,'Bottle':2,'Bus':3,'Car':4,'Cat':5,'Chair':6,'Cup':7,'Dog':8,'Motorbike':9,'People':10,'Table':11}\n",
        "\n",
        "for train_key, train_val in hist_train.items():\n",
        "  for val in train_val:\n",
        "    X_train.append(val)\n",
        "    y_train.append(dict_cat[train_key]) \n",
        "\n",
        "for test_key, test_val in hist_test.items():\n",
        "  for val in test_val:\n",
        "    X_test.append(val)\n",
        "    y_test.append(dict_cat[test_key]) \n",
        "\n",
        "# Features normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale = StandardScaler()\n",
        "\n",
        "X_train_norm = featureNormalization(scale, X_train)\n",
        "X_test_norm = featureNormalization(scale, X_test)"
      ],
      "metadata": {
        "id": "1sQOqWkMGs82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "vj8-l8CQNy6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "def SVC_train(X_train, y_train, kernel):\n",
        "  features = X_train\n",
        "  Cs = [0.5, 0.1, 0.15, 0.2, 0.3]\n",
        "  gammas = [0.1, 0.11, 0.095, 0.105]\n",
        "  param_grid = {'C': Cs, 'gamma' : gammas}\n",
        "  grid_search = GridSearchCV(SVC(kernel=kernel), param_grid, cv=5)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  params =  grid_search.best_params_\n",
        "\n",
        "  C_param, gamma_param = params.get(\"C\"), params.get(\"gamma\")\n",
        "\n",
        "  svm = SVC(kernel = kernel, C =  C_param, gamma = gamma_param, class_weight = None)\n",
        "  svm.fit(X_train, y_train)\n",
        "  print('Best Accuracy Through Grid Search : %.3f'%grid_search.best_score_)\n",
        "  print('Best Parameters : ',grid_search.best_params_)\n",
        "  print('Train Accuracy : %.3f'%svm.score(X_train, y_train))\n",
        "\n",
        "  return svm\n",
        "\n",
        "def SVC_test(svm, X_test, y_test):\n",
        "  print('Test Accuracy : %.3f'%svm.score(X_test, y_test))\n",
        "\n",
        "def plot_conf_matr(svm, X_test, y_test):\n",
        "  # Generate confusion matrix\n",
        "  matrix = plot_confusion_matrix(svm, X_test, y_test,\n",
        "                                  cmap=plt.cm.Blues,\n",
        "                                  normalize='true')\n",
        "  plt.title('Confusion matrix for SVM classifier')\n",
        "  plt.show(matrix)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "9d0GQCBpQ_8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_rbf = SVC_train(X_train, y_train, 'rbf')\n",
        "SVC_test(svm_rbf, X_test, y_test)\n",
        "plot_conf_matr(svm_rbf, X_test, y_test)"
      ],
      "metadata": {
        "id": "tM6WFkhIrqDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_linear = SVC_train(X_train, y_train, 'linear')\n",
        "SVC_test(svm_linear, X_test, y_test)\n",
        "plot_conf_matr(svm_linear, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "Dh2snTydrp_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_polynomial  = SVC_train(X_train, y_train, 'poly')\n",
        "SVC_test(svm_polynomial, X_test, y_test)\n",
        "plot_conf_matr(svm_polynomial, X_test, y_test)"
      ],
      "metadata": {
        "id": "OypPcQfarp7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}